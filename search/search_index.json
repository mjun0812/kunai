{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Home</p>"},{"location":"hydra_utils/","title":"Hydra utils","text":""},{"location":"hydra_utils/#kunai.hydra_utils.enum_dict_keys","title":"<code>enum_dict_keys(base, base_name='')</code>","text":"<p>dict\u306ekey\u3092\u518d\u5e30\u7684\u306b\u5217\u6319\u3057\u305f\u30ea\u30b9\u30c8\u3092\u53d6\u5f97\u3059\u308b</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>dict</code> <p>\u5217\u6319\u3059\u308b\u30ea\u30b9\u30c8</p> required <code>base_name</code> <code>str</code> <p>\u89aa\u306e\u968e\u5c64\u306ekey\uff0e\u518d\u8d77\u5b9f\u884c\u7528. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>dict\u306ekey\u306e\u30ea\u30b9\u30c8</p> Source code in <code>kunai/hydra_utils/hydra_utils.py</code> <pre><code>def enum_dict_keys(base, base_name=\"\") -&gt; list:\n    \"\"\"dict\u306ekey\u3092\u518d\u5e30\u7684\u306b\u5217\u6319\u3057\u305f\u30ea\u30b9\u30c8\u3092\u53d6\u5f97\u3059\u308b\n\n    Args:\n        base (dict): \u5217\u6319\u3059\u308b\u30ea\u30b9\u30c8\n        base_name (str, optional): \u89aa\u306e\u968e\u5c64\u306ekey\uff0e\u518d\u8d77\u5b9f\u884c\u7528. Defaults to \"\".\n\n    Returns:\n        list: dict\u306ekey\u306e\u30ea\u30b9\u30c8\n    \"\"\"\n    key_list = []\n    for key in base.keys():\n        if base_name:\n            key_name = base_name + \".\" + key\n        else:\n            key_name = key\n        if isinstance(base[key], dict):\n            key_list.extend(enum_dict_keys(base[key], base_name=key_name))\n        else:\n            key_list.append(key_name)\n    return key_list\n</code></pre>"},{"location":"hydra_utils/#kunai.hydra_utils.get_default_config","title":"<code>get_default_config(config_dir_path, config_name='config.yaml')</code>","text":"<p>hydra\u3092\u4f7f\u308f\u305a\u306b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u5206\u5272\u3055\u308c\u305fyaml\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092OmegaConf\u306b\u5909\u63db\u3059\u308b</p> <p>\uff12\u3064\u4ee5\u4e0a\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u6bd4\u8f03\u3059\u308b\u3068\u304d\u306b\u5fc5\u8981</p> <p>Parameters:</p> Name Type Description Default <code>config_dir_path</code> <code>str</code> <p>config\u7fa4\u304c\u3042\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea</p> required <code>config_name</code> <code>str</code> <p>config\u306e\u30d5\u30a1\u30a4\u30eb\u540d. Defaults to \"config.yaml\".</p> <code>'config.yaml'</code> <p>Returns:</p> Type Description <code>OmegaConf</code> <p>OmegaConf.omegaconf: omegaconf</p> Source code in <code>kunai/hydra_utils/hydra_utils.py</code> <pre><code>def get_default_config(config_dir_path, config_name=\"config.yaml\") -&gt; OmegaConf:\n    \"\"\"hydra\u3092\u4f7f\u308f\u305a\u306b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u5206\u5272\u3055\u308c\u305fyaml\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092OmegaConf\u306b\u5909\u63db\u3059\u308b\n\n    \uff12\u3064\u4ee5\u4e0a\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u6bd4\u8f03\u3059\u308b\u3068\u304d\u306b\u5fc5\u8981\n\n    Args:\n        config_dir_path (str): config\u7fa4\u304c\u3042\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\n        config_name (str, optional): config\u306e\u30d5\u30a1\u30a4\u30eb\u540d. Defaults to \"config.yaml\".\n\n    Returns:\n        OmegaConf.omegaconf: omegaconf\n    \"\"\"\n    # get default root config\n    with open(os.path.join(config_dir_path, config_name), \"r\") as f:\n        cfg = yaml.safe_load(f)\n    default = {}\n    for d in cfg[\"defaults\"]:\n        try:\n            default.update(d)\n        except Exception:\n            pass\n\n    # get nest config key\n    file_list = os.listdir(config_dir_path)\n    dir_list = [f for f in file_list if os.path.isdir(os.path.join(config_dir_path, f))]\n\n    for dir_name in dir_list:\n        cfg_name = default[dir_name]\n        with open(os.path.join(config_dir_path, dir_name, cfg_name + \".yaml\")) as f:\n            split_cfg = yaml.safe_load(f)\n        cfg[dir_name] = split_cfg\n    cfg = OmegaConf.create(cfg)\n\n    return cfg\n</code></pre>"},{"location":"hydra_utils/#kunai.hydra_utils.set_hydra","title":"<code>set_hydra(cfg, verbose=True, writable=True)</code>","text":"<p>config\u30d5\u30a1\u30a4\u30eb\u3092\u6a19\u6e96\u51fa\u529b\u3057\uff0cCurrent Dir\u3092\u5b9f\u884c\u30b9\u30af\u30ea\u30d7\u30c8\u306e\u5834\u6240\u306b\u623b\u3059\uff0e</p> <p>hydra\u306f\u4f7f\u7528\u6642\u306b\u30ab\u30ec\u30f3\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u5909\u66f4\u3059\u308b\u305f\u3081\uff0c hydra.utils.get_original_cwd\u3092\u4f7f\u3063\u3066\u5b9f\u884c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u53d6\u5f97\u3057\u3066 \u3082\u3068\u306b\u623b\u3059\uff0e</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>omegaconf</code> <p>Hydra config obj.</p> required <code>verbose</code> <code>bool</code> <p>\u30b3\u30f3\u30d5\u30a3\u30b0\u3092\u6a19\u6e96\u51fa\u529b\u3059\u308b</p> <code>True</code> <code>writable</code> <code>bool</code> <p>\u65b0\u3057\u3044\u5024\u3092\u8ffd\u52a0\u53ef\u80fd\u306b\u3059\u308b</p> <code>True</code> Source code in <code>kunai/hydra_utils/hydra_utils.py</code> <pre><code>@is_available\ndef set_hydra(cfg, verbose=True, writable=True):\n    \"\"\"config\u30d5\u30a1\u30a4\u30eb\u3092\u6a19\u6e96\u51fa\u529b\u3057\uff0cCurrent Dir\u3092\u5b9f\u884c\u30b9\u30af\u30ea\u30d7\u30c8\u306e\u5834\u6240\u306b\u623b\u3059\uff0e\n\n    hydra\u306f\u4f7f\u7528\u6642\u306b\u30ab\u30ec\u30f3\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u5909\u66f4\u3059\u308b\u305f\u3081\uff0c\n    hydra.utils.get_original_cwd\u3092\u4f7f\u3063\u3066\u5b9f\u884c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u53d6\u5f97\u3057\u3066\n    \u3082\u3068\u306b\u623b\u3059\uff0e\n\n    Args:\n        cfg (OmegaConf.omegaconf): Hydra config obj.\n        verbose (bool): \u30b3\u30f3\u30d5\u30a3\u30b0\u3092\u6a19\u6e96\u51fa\u529b\u3059\u308b\n        writable (bool): \u65b0\u3057\u3044\u5024\u3092\u8ffd\u52a0\u53ef\u80fd\u306b\u3059\u308b\n    \"\"\"\n    if verbose:\n        # Print config\n        print(OmegaConf.to_yaml(cfg))\n    if writable:\n        OmegaConf.set_struct(cfg, False)\n    # Hydra\u306fcurrent directry\u3092\u5b9f\u884c\u5834\u6240\u304b\u3089\u5909\u66f4\u3059\u308b\u306e\u3067\u3082\u3068\u306b\u623b\u3057\u3066\u304a\u304f\n    os.chdir(hydra.utils.get_original_cwd())\n</code></pre>"},{"location":"hydra_utils/#kunai.hydra_utils.validate_config","title":"<code>validate_config(cfg)</code>","text":"<p>2\u3064\u306eOmegaConf\u3092\u30de\u30fc\u30b8\u3059\u308b\uff0e</p> <p>main\u95a2\u6570\u3067\u30ed\u30fc\u30c9\u3057\u305f\u3082\u306e\u3068\uff0c\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u4f7f\u3063\u3066\u3044\u308b\u3082\u306e\u3092\u30de\u30fc\u30b8\u3059\u308b\uff0e \u958b\u767a\u4e2d\u306b\u30c7\u30d5\u30a9\u30eb\u30c8\u306econfig\u306b\u8ffd\u52a0\u304c\u3042\u3063\u305f\u5834\u5408\u306b\uff0c\u65e2\u306b\u751f\u6210\u3055\u308c\u305fconfig\u306b \u8ffd\u52a0\u3057\u305f\u9805\u76ee\u3092\u30ed\u30fc\u30c9\u3057\u305fconfig\u306b\u8ffd\u52a0\u3059\u308b\u305f\u3081\u306b\u4f7f\u3046</p> <pre><code># load config\nHOGE: 3\n\n# default config\nHOGE: 2\nHUGA: 1\n\n# merged config\nHOGE: 3\nHUGA: 1\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>omegaconf</code> <p>\u30ed\u30fc\u30c9\u3057\u305fconfig\u30d5\u30a1\u30a4\u30eb</p> required <p>Returns:</p> Type Description <code>OmegaConf</code> <p>OmegaConf.omegaconf: \u30de\u30fc\u30b8\u3055\u308c\u305fconfig\u30d5\u30a1\u30a4\u30eb</p> Source code in <code>kunai/hydra_utils/hydra_utils.py</code> <pre><code>def validate_config(cfg) -&gt; OmegaConf:\n    \"\"\"2\u3064\u306eOmegaConf\u3092\u30de\u30fc\u30b8\u3059\u308b\uff0e\n\n    main\u95a2\u6570\u3067\u30ed\u30fc\u30c9\u3057\u305f\u3082\u306e\u3068\uff0c\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u4f7f\u3063\u3066\u3044\u308b\u3082\u306e\u3092\u30de\u30fc\u30b8\u3059\u308b\uff0e\n    \u958b\u767a\u4e2d\u306b\u30c7\u30d5\u30a9\u30eb\u30c8\u306econfig\u306b\u8ffd\u52a0\u304c\u3042\u3063\u305f\u5834\u5408\u306b\uff0c\u65e2\u306b\u751f\u6210\u3055\u308c\u305fconfig\u306b\n    \u8ffd\u52a0\u3057\u305f\u9805\u76ee\u3092\u30ed\u30fc\u30c9\u3057\u305fconfig\u306b\u8ffd\u52a0\u3059\u308b\u305f\u3081\u306b\u4f7f\u3046\n\n    ```yaml\n    # load config\n    HOGE: 3\n\n    # default config\n    HOGE: 2\n    HUGA: 1\n\n    # merged config\n    HOGE: 3\n    HUGA: 1\n    ```\n\n    Args:\n        cfg (OmegaConf.omegaconf): \u30ed\u30fc\u30c9\u3057\u305fconfig\u30d5\u30a1\u30a4\u30eb\n\n    Returns:\n        OmegaConf.omegaconf: \u30de\u30fc\u30b8\u3055\u308c\u305fconfig\u30d5\u30a1\u30a4\u30eb\n    \"\"\"\n    default_cfg = get_default_config(\"./config\")\n    default_keys = enum_dict_keys(OmegaConf.to_container(default_cfg, resolve=True))\n    cfg_keys = enum_dict_keys(OmegaConf.to_container(cfg, resolve=True))\n    if set(default_keys) - set(cfg_keys):\n        logger.warning(f\"input config nothing keys: {set(default_keys) - set(cfg_keys)}\")\n    if set(cfg_keys) - set(default_keys):\n        logger.warning(f\"not using keys input config: {set(cfg_keys) - set(default_keys)}\")\n\n    merge = OmegaConf.merge(default_cfg, cfg)\n    return merge\n</code></pre>"},{"location":"torch_utils/","title":"Torch utils","text":"<p>kunai.torch</p>"},{"location":"torch_utils/#kunai.torch_utils.calc_params","title":"<code>calc_params(params_dict)</code>","text":"<p>Calculate number of parameters</p> <p>Parameters:</p> Name Type Description Default <code>params_dict</code> <code>dict</code> <p>PyTorch model state_dict</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>num params</p> Source code in <code>kunai/torch_utils/model_util.py</code> <pre><code>@is_available\ndef calc_params(params_dict: dict) -&gt; int:\n    \"\"\"Calculate number of parameters\n\n    Args:\n        params_dict (dict): PyTorch model state_dict\n\n    Returns:\n        int: num params\n    \"\"\"\n    num_params = 0\n    for v in params_dict.values():\n        n = 1\n        for p in v.shape:\n            n *= p\n        num_params += n\n    return num_params\n</code></pre>"},{"location":"torch_utils/#kunai.torch_utils.check_model_parallel","title":"<code>check_model_parallel(model)</code>","text":"<p>check model is parallel or single</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model file</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>parallel = True, single = False</p> Source code in <code>kunai/torch_utils/model_util.py</code> <pre><code>@is_available\ndef check_model_parallel(model) -&gt; bool:\n    \"\"\"check model is parallel or single\n\n    Args:\n        model (torch.nn.Module): Model file\n\n    Returns:\n        bool: parallel = True, single = False\n    \"\"\"\n    return isinstance(model, torch.nn.DataParallel) or isinstance(model, DistributedDataParallel)\n</code></pre>"},{"location":"torch_utils/#kunai.torch_utils.cuda_info","title":"<code>cuda_info(global_cuda_index=0)</code>","text":"<p>show using GPU Info</p> <p>Parameters:</p> Name Type Description Default <code>global_cuda_index</code> <code>int</code> <p>using GPU number in all GPU number. Defaults to 0.</p> <code>0</code> Source code in <code>kunai/torch_utils/cuda.py</code> <pre><code>@is_available\ndef cuda_info(global_cuda_index=0):\n    \"\"\"show using GPU Info\n\n    Args:\n        global_cuda_index (int, optional): using GPU number in all GPU number. Defaults to 0.\n    \"\"\"\n    for i in range(torch.cuda.device_count()):\n        info = torch.cuda.get_device_properties(i)\n        print(f\"CUDA:{i + global_cuda_index} {info.name}, {info.total_memory / 1024 ** 2}MB\")\n</code></pre>"},{"location":"torch_utils/#kunai.torch_utils.fix_seed","title":"<code>fix_seed(seed)</code>","text":"<p>fix seed on random, numpy, torch module</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>seed parameter</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>seed parameter</p> Source code in <code>kunai/torch_utils/seed.py</code> <pre><code>@is_available\ndef fix_seed(seed) -&gt; int:\n    \"\"\"fix seed on random, numpy, torch module\n\n    Args:\n        seed (int): seed parameter\n\n    Returns:\n        int: seed parameter\n    \"\"\"\n    # random\n    random.seed(seed)\n    # Numpy\n    np.random.seed(seed)\n    # Pytorch\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    return seed\n</code></pre>"},{"location":"torch_utils/#kunai.torch_utils.prety_print_model_param","title":"<code>prety_print_model_param(state_dict)</code>","text":"<p>Print model parameters</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>dict</code> <p>PyTorch model state_dict</p> required Source code in <code>kunai/torch_utils/model_util.py</code> <pre><code>@is_available\ndef prety_print_model_param(state_dict: dict) -&gt; None:\n    \"\"\"Print model parameters\n\n    Args:\n        state_dict (dict): PyTorch model state_dict\n    \"\"\"\n    parsed_dict = {}\n    for key, param in state_dict.items():\n        keys = key.split(\".\")\n\n        tmp_dict = parsed_dict\n        for k in keys[:-1]:\n            tmp_dict[k] = tmp_dict.get(k, {})\n            tmp_dict = tmp_dict[k]\n        num_params = 0\n        for p in param.shape:\n            num_params *= p\n        tmp_dict[keys[-1]] = f\"{list(param.shape)} {num_params}\"\n    pprint(parsed_dict)\n</code></pre>"},{"location":"torch_utils/#kunai.torch_utils.save_model","title":"<code>save_model(model, file_path)</code>","text":"<p>Save PyTorch Model PyTorch\u306e\u30e2\u30c7\u30eb\u3092\u4fdd\u5b58\u3059\u308b Parallel\u306b\u3082\u5bfe\u5fdc</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>\u30e2\u30c7\u30eb\u30aa\u30d6\u30b8\u30a7\u30af\u30c8</p> required <code>file_path</code> <code>str</code> <p>\u4fdd\u5b58\u5148</p> required Source code in <code>kunai/torch_utils/model_util.py</code> <pre><code>@is_available\ndef save_model(model, file_path):\n    \"\"\"Save PyTorch Model\n    PyTorch\u306e\u30e2\u30c7\u30eb\u3092\u4fdd\u5b58\u3059\u308b\n    Parallel\u306b\u3082\u5bfe\u5fdc\n\n    Args:\n        model (torch.nn.Module): \u30e2\u30c7\u30eb\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\n        file_path (str): \u4fdd\u5b58\u5148\n    \"\"\"\n\n    if check_model_parallel(model):\n        model = model.module\n    torch.save(model.state_dict(), file_path)\n    logger.info(\"Saving model at %s\", file_path)\n</code></pre>"},{"location":"torch_utils/#kunai.torch_utils.save_model_info","title":"<code>save_model_info(output_dir, model, input_size=None, input_data=None, prefix='')</code>","text":"<p>Output PyTorch Model Summary to log.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>string</code> <p>output log dir</p> required <code>model</code> <code>Module</code> <p>PyTorch Model Class</p> required <code>input_size</code> <code>List</code> <p>input tensor size</p> <code>None</code> <code>input_data</code> <code>List[Tensor]</code> <p>input data</p> <code>None</code> <code>prefix</code> <code>str</code> <p>log file prefix output_dir/model_summary_{prefix}.log. Defaults to \"\".</p> <code>''</code> Source code in <code>kunai/torch_utils/model_util.py</code> <pre><code>@is_available\ndef save_model_info(output_dir, model, input_size=None, input_data=None, prefix=\"\"):\n    \"\"\"Output PyTorch Model Summary to log.\n\n    Args:\n        output_dir (string): output log dir\n        model (torch.nn.Module): PyTorch Model Class\n        input_size (List): input tensor size\n        input_data (List[Tensor]): input data\n        prefix (str, optional): log file prefix output_dir/model_summary_{prefix}.log. Defaults to \"\".\n    \"\"\"\n\n    if prefix:\n        prefix = \"_\" + prefix\n    if check_model_parallel(model):\n        model = model.module\n\n    device = next(model.parameters()).device\n\n    if input_size is None:\n        model_summary = str(summary(model, input_data=input_data, device=device, verbose=0))\n    elif input_data is None:\n        model_summary = str(summary(model, input_size=input_size, device=device, verbose=0))\n    else:\n        model_summary = str(summary(model, device=device, verbose=0))\n\n    # Model Summary\n    with open(os.path.join(output_dir, f\"model_summary{prefix}.log\"), \"a\") as f:\n        print(model, file=f)\n        print(model_summary, file=f)\n</code></pre>"},{"location":"torch_utils/#kunai.torch_utils.set_device","title":"<code>set_device(global_gpu_index, rank=-1, is_cpu=False, use_cudnn=True, cudnn_deterministic=False, pci_device_order=True, verbose=True)</code>","text":"<p>Set use GPU or CPU Device</p> <p>set using GPU or CPU Device(instead of CUDA_VISIBLE_DEVICES). set also CUDNN.</p> <p>Parameters:</p> Name Type Description Default <code>global_gpu_index</code> <code>int</code> <p>using gpu number in all gpu.</p> required <code>rank</code> <code>int</code> <p>process rank</p> <code>-1</code> <code>is_cpu</code> <code>bool</code> <p>use cpu or not. Defaults to False.</p> <code>False</code> <code>pci_device_order</code> <code>bool</code> <p>. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <p>torch.device: use device object.</p> Source code in <code>kunai/torch_utils/cuda.py</code> <pre><code>@is_available\ndef set_device(\n    global_gpu_index,\n    rank=-1,\n    is_cpu=False,\n    use_cudnn=True,\n    cudnn_deterministic=False,\n    pci_device_order=True,\n    verbose=True,\n):\n    \"\"\"Set use GPU or CPU Device\n\n    set using GPU or CPU Device(instead of CUDA_VISIBLE_DEVICES).\n    set also CUDNN.\n\n    Args:\n        global_gpu_index (int): using gpu number in all gpu.\n        rank (int): process rank\n        is_cpu (bool, optional): use cpu or not. Defaults to False.\n        pci_device_order (bool, optional): . Defaults to True.\n\n    Returns:\n        torch.device: use device object.\n    \"\"\"\n\n    if not is_cpu:\n        if pci_device_order:\n            os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(global_gpu_index)\n\n        # print using GPU Info\n        if verbose:\n            cuda_info(int(os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\")[0]))\n            print(f\"Using GPU is CUDA:{global_gpu_index}\")\n\n        if use_cudnn and cudnn.is_available():\n            cudnn.benchmark = True\n            cudnn.deterministic = cudnn_deterministic  # \u4e71\u6570\u56fa\u5b9a\u306e\u305f\u3081\n            if verbose:\n                print(\"Use CUDNN\")\n        if rank == -1:\n            rank = 0\n        device = torch.device(rank)\n        torch.cuda.set_device(rank)\n    else:\n        device = torch.device(\"cpu\")\n        if verbose:\n            print(\"Use CPU\")\n\n    return device\n</code></pre>"},{"location":"torch_utils/#kunai.torch_utils.time_synchronized","title":"<code>time_synchronized()</code>","text":"<p>return time at synhronized CUDA and CPU.    CUDA\u3068CPU\u306e\u8a08\u7b97\u304c\u975e\u540c\u671f\u306a\u305f\u3081\uff0c\u540c\u671f\u3057\u3066\u304b\u3089\u6642\u9593\u8a08\u7b97\u3059\u308b\uff0e</p> <p>Returns:</p> Name Type Description <code>time</code> <code>time</code> <p>\u95a2\u6570\u547c\u3073\u51fa\u3057\u6642\u306e\u6642\u523b</p> Source code in <code>kunai/torch_utils/cuda.py</code> <pre><code>@is_available\ndef time_synchronized() -&gt; time:\n    \"\"\"return time at synhronized CUDA and CPU.\n       CUDA\u3068CPU\u306e\u8a08\u7b97\u304c\u975e\u540c\u671f\u306a\u305f\u3081\uff0c\u540c\u671f\u3057\u3066\u304b\u3089\u6642\u9593\u8a08\u7b97\u3059\u308b\uff0e\n\n    Returns:\n        time: \u95a2\u6570\u547c\u3073\u51fa\u3057\u6642\u306e\u6642\u523b\n    \"\"\"\n    # pytorch-accurate time\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    return time.time()\n</code></pre>"},{"location":"torch_utils/#kunai.torch_utils.worker_init_fn","title":"<code>worker_init_fn(worker_id)</code>","text":"<p>Reset numpy random seed in PyTorch Dataloader</p> <p>Parameters:</p> Name Type Description Default <code>worker_id</code> <code>int</code> <p>random seed value</p> required Source code in <code>kunai/torch_utils/seed.py</code> <pre><code>@is_available\ndef worker_init_fn(worker_id):\n    \"\"\"Reset numpy random seed in PyTorch Dataloader\n\n    Args:\n        worker_id (int): random seed value\n    \"\"\"\n    # random\n    # random.seed(random.getstate()[1][0] + worker_id)\n    # Numpy\n    np.random.seed(np.random.get_state()[1][0] + worker_id)\n</code></pre>"},{"location":"utils/","title":"Utils","text":""}]}